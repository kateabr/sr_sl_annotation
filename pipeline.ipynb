{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Step 1: run `pip install classla` to install [classla](https://github.com/clarinsi/classla)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 2: download standard models and initialize pipelines"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import classla\n",
    "\n",
    "# slovenian\n",
    "classla.download('sl')\n",
    "sl_nlp = classla.Pipeline('sl')\n",
    "\n",
    "# serbian\n",
    "classla.download('sr')\n",
    "sr_nlp = classla.Pipeline('sr')\n",
    "\n",
    "# serbian non-standard\n",
    "classla.download('sr', type='nonstandard')\n",
    "sr_nlp_nonstandard = classla.Pipeline('sr', type='nonstandard')\n",
    "\n",
    "# croatian\n",
    "classla.download('hr')\n",
    "hr_nlp = classla.Pipeline('hr')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 3: specify input directories and match them with corresponding pipeline; check well-formedness of the files and correct if needed (only well-formed xmls will be processed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from pathlib import Path\n",
    "\n",
    "STD_SUFF = \"\"\n",
    "NONSTD_SUFF = \"_nonstd\"\n",
    "MERGED_SUFF = \"_merged\"\n",
    "\n",
    "LANG_SERBIAN = \"srp\"\n",
    "LANG_SLOVENIAN = \"slv\"\n",
    "LANG_RUSSIAN = \"rus\"\n",
    "LANG_CROATIAN = \"hrv\"\n",
    "\n",
    "LANG_CODE_NORMALIZED = {\n",
    "    LANG_SLOVENIAN: LANG_SLOVENIAN,\n",
    "    \"sln\": LANG_SLOVENIAN,\n",
    "    LANG_SERBIAN: LANG_SERBIAN,\n",
    "    LANG_CROATIAN: LANG_CROATIAN,\n",
    "}\n",
    "\n",
    "source_dir = Path(\"source\")\n",
    "\n",
    "pipelines = {\n",
    "    LANG_SLOVENIAN: sl_nlp,\n",
    "    LANG_SERBIAN: sr_nlp,\n",
    "    LANG_CROATIAN: hr_nlp\n",
    "}\n",
    "\n",
    "pipelines_nonstd = {\n",
    "    LANG_SERBIAN: sr_nlp_nonstandard\n",
    "}\n",
    "\n",
    "files_to_process = []\n",
    "\n",
    "def check_directory(directory_nested):\n",
    "    for file in Path(directory_nested).iterdir():\n",
    "        if not file.is_file():\n",
    "            check_directory(file)\n",
    "            continue\n",
    "        try:\n",
    "            etree.parse(file)\n",
    "            print(f\"✅ File {file} is well-formed.\")\n",
    "            files_to_process.append(file)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ File {file} is NOT well-formed!\\n{e}\")\n",
    "\n",
    "check_directory(source_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 4.1: load [markdown mappings](https://docs.google.com/spreadsheets/d/1PiNjHQ7NoJyYLiTin4VehWd21Tc7LLdoCr52-t9_EjY) and initialize mapping function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mapping_sheet_id = \"1PiNjHQ7NoJyYLiTin4VehWd21Tc7LLdoCr52-t9_EjY\"\n",
    "mapping_sheet_names = {LANG_SLOVENIAN: \"sl\", LANG_SERBIAN: \"srp\", LANG_CROATIAN: \"hr\"}\n",
    "\n",
    "def mapping_sheet_to_dict(lang):\n",
    "    mapping_sheet_dict = pd.read_csv(f\"https://docs.google.com/spreadsheets/d/{mapping_sheet_id}/gviz/tq?tqx=out:csv&sheet={mapping_sheet_names[lang]}\", na_filter= False).to_dict()\n",
    "    src = list(mapping_sheet_dict[\"classla\"].values())\n",
    "    trg = list(map(lambda x: x if len(x) > 0 else None, list(mapping_sheet_dict[\"ncrl\"].values())))\n",
    "    return {src_mapping: trg_mapping for src_mapping, trg_mapping in zip(src, trg)}\n",
    "\n",
    "gr_map = {\n",
    "    LANG_SLOVENIAN: mapping_sheet_to_dict(LANG_SLOVENIAN),\n",
    "    LANG_SERBIAN: mapping_sheet_to_dict(LANG_SERBIAN),\n",
    "    LANG_CROATIAN: mapping_sheet_to_dict(LANG_CROATIAN),\n",
    "}\n",
    "\n",
    "additional_ncrl_features = {\"ANUM\", \"NONLEX\", \"gvrn:acc\", \"gvrn:dat\", \"gvrn:gen\", \"gvrn:ins\", \"gvrn:loc\", \"gvrn:nom\", \"gvrn:voc\"}\n",
    "ncrl_features = set([])\n",
    "for dct in gr_map.values():\n",
    "    ncrl_features = ncrl_features.union(dct.values())\n",
    "ncrl_features = additional_ncrl_features.union(sum([val.split(\",\") for val in ncrl_features if val is not None], []))\n",
    "\n",
    "def get_mapping(lang, prop):\n",
    "    return gr_map[lang][prop]\n",
    "\n",
    "def is_mapped(lang, prop):\n",
    "    return prop in gr_map[lang].keys()\n",
    "\n",
    "def is_ncrl_feature(prop):\n",
    "    return prop in ncrl_features\n",
    "\n",
    "\n",
    "def map_to_ncrl(lang, pos, attrs):\n",
    "    if not is_mapped(lang, pos):\n",
    "        print(f\"[{lang}] unknown POS: {pos}\")\n",
    "        res_pos = pos\n",
    "    else:\n",
    "        res_pos = get_mapping(lang, pos)\n",
    "    if attrs is None:\n",
    "        return res_pos, None\n",
    "    res_attrs = set([])\n",
    "    for attr_pair_str in attrs.lower().split(\"|\"):\n",
    "        if attr_pair_str == \"numtype=ord\" and res_pos == \"A\":\n",
    "            res_pos = \"ANUM\"\n",
    "        elif attr_pair_str == \"foreign=yes\":\n",
    "            res_pos = \"NONLEX\"\n",
    "        elif attr_pair_str.startswith(\"case=\") and res_pos == \"PR\":\n",
    "            res_attrs.add(f\"gvrn:{get_mapping(lang, attr_pair_str)}\")\n",
    "        elif not is_mapped(lang, attr_pair_str):\n",
    "            print(f\"[{lang}] unmapped attribute pair: {attr_pair_str}\")\n",
    "            res_attrs.add(attr_pair_str)\n",
    "        elif (mapping := get_mapping(lang, attr_pair_str)) is not None:\n",
    "            res_attrs.add(mapping)\n",
    "    if len(res_attrs) > 0:\n",
    "        return res_pos, \",\".join(res_attrs)\n",
    "    else:\n",
    "        return res_pos, None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 4.2: initialize file processing functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import cyrtranslit\n",
    "from tqdm import tqdm\n",
    "\n",
    "# processed files will be exported to this folder\n",
    "STORE_RESULTS_AT = \"annotated\"\n",
    "\n",
    "known_keys = {\"feats\"}\n",
    "keys_to_ignore = {\"id\", \"text\", \"lemma\", \"upos\", \"xpos\", \"head\", \"ner\", \"deprel\"}\n",
    "non_textual_pos = {\"PUNCT\", \"X\", \"SYM\"}\n",
    "\n",
    "def add_space_after(token_dict):\n",
    "    return not (\"misc\" in token_dict.keys() and \"SpaceAfter=No\" in token_dict[\"misc\"])\n",
    "\n",
    "def annotate(lang, ana, token_dict):\n",
    "    for key, value in token_dict.items():\n",
    "        if key not in keys_to_ignore and key not in known_keys and (key != \"misc\" and value != \"SpaceAfter=No\"):\n",
    "            print(f\"{key} :: {value} in {token_dict}\")\n",
    "    if \"feats\" in token_dict.keys():\n",
    "        pos, attrs = map_to_ncrl(lang, token_dict[\"upos\"], token_dict[\"feats\"])\n",
    "        if attrs is not None:\n",
    "            ana.attrib[\"gr\"] = f\"{pos},{attrs}\"\n",
    "        else:\n",
    "            ana.attrib[\"gr\"] = pos\n",
    "    else:\n",
    "        pos, _ = map_to_ncrl(lang, token_dict[\"upos\"], None)\n",
    "        ana.attrib[\"gr\"] = pos\n",
    "\n",
    "def append_text(root, text):\n",
    "    if len(root.getchildren()) > 0:\n",
    "        node = last_child(root)\n",
    "        if node.tail is None:\n",
    "            node.tail = text\n",
    "        else:\n",
    "            node.tail += text\n",
    "    else:\n",
    "        if root.text is None:\n",
    "            root.text = text\n",
    "        else:\n",
    "            root.text += text\n",
    "\n",
    "def last_child(node):\n",
    "    return node.getchildren()[-1]\n",
    "\n",
    "def get_word(token):\n",
    "    return last_child(token).tail.strip()\n",
    "\n",
    "def fill_text_and_lex(ana, token_dict, lang, is_latin):\n",
    "    word_token = ana.getparent()\n",
    "    if lang == LANG_SERBIAN:\n",
    "        if token_dict[\"xpos\"].startswith(\"Pp3f\"):\n",
    "            if is_latin:\n",
    "                token_dict[\"lemma\"] = \"ona\"\n",
    "            else:\n",
    "                token_dict[\"lemma\"] = \"она\"\n",
    "        elif token_dict[\"xpos\"].startswith(\"Pp3n\"):\n",
    "            if is_latin:\n",
    "                token_dict[\"lemma\"] = \"ono\"\n",
    "            else:\n",
    "                token_dict[\"lemma\"] = \"оно\"\n",
    "        if is_latin:\n",
    "            ana.tail = token_dict[\"text\"]\n",
    "            word_token.attrib[\"translit\"] = cyrtranslit.to_cyrillic(token_dict[\"text\"], \"sr\")\n",
    "            ana.attrib[\"lex\"] = token_dict[\"lemma\"]\n",
    "            ana.attrib[\"lex_translit\"] = cyrtranslit.to_cyrillic(token_dict[\"lemma\"], \"sr\")\n",
    "        else:\n",
    "            ana.tail = cyrtranslit.to_cyrillic(token_dict[\"text\"], \"sr\")\n",
    "            word_token.attrib[\"translit\"] = token_dict[\"text\"]\n",
    "            ana.attrib[\"lex\"] = cyrtranslit.to_cyrillic(token_dict[\"lemma\"], \"sr\")\n",
    "            ana.attrib[\"lex_translit\"] = token_dict[\"lemma\"]\n",
    "    else:\n",
    "        ana.tail = token_dict[\"text\"]\n",
    "        ana.attrib[\"lex\"] = token_dict[\"lemma\"]\n",
    "\n",
    "def recreate_original_filepath(filename):\n",
    "    try:\n",
    "        if not Path(STORE_RESULTS_AT).joinpath(filename.parent).exists():\n",
    "            new_filepath = Path(STORE_RESULTS_AT)\n",
    "            for part in filename.parts:\n",
    "                if part == filename.name:\n",
    "                    break\n",
    "                new_filepath = new_filepath.joinpath(part)\n",
    "                if not Path(new_filepath).exists():\n",
    "                    Path.mkdir(new_filepath)\n",
    "    except Exception as _:\n",
    "        Path.mkdir(Path(STORE_RESULTS_AT).joinpath(filename.parent))\n",
    "\n",
    "def format_sentence(sentence):\n",
    "    previous_variant = sentence.getprevious()\n",
    "    if previous_variant is not None:\n",
    "        if previous_variant.tail is None:\n",
    "            previous_variant.tail = \"\\n    \"\n",
    "        else:\n",
    "            previous_variant.tail = previous_variant.tail.strip() + \"\\n    \"\n",
    "\n",
    "def add_suffix(filename, file_suff):\n",
    "    return filename.parent.joinpath(filename.stem + file_suff + filename.suffix)\n",
    "\n",
    "def process_para(para, pipelines):\n",
    "    for sentence in para:\n",
    "        if sentence.tag != \"se\":\n",
    "            print(f\"Incorrect sentence tag: {sentence.tag}\")\n",
    "        lang = sentence.get(\"lang\")\n",
    "        if lang not in LANG_CODE_NORMALIZED.keys() or sentence.text is None:\n",
    "            format_sentence(sentence)\n",
    "            continue\n",
    "        lang = LANG_CODE_NORMALIZED[lang]\n",
    "        if lang not in pipelines.keys():\n",
    "            format_sentence(sentence)\n",
    "            continue\n",
    "        tokenized_sentence = etree.Element(\"se\", attrib={\"lang\": lang})\n",
    "        if lang == LANG_SERBIAN:\n",
    "            latin_sentence = cyrtranslit.to_latin(sentence.text, \"sr\")\n",
    "            parsed_sentence = pipelines[lang](latin_sentence)\n",
    "            tokenized_sentence.attrib[\"has_translit\"] = \"true\"\n",
    "        else:\n",
    "            parsed_sentence = pipelines[lang](sentence.text)\n",
    "        if all(token.to_dict()[0][\"upos\"] in non_textual_pos for token in parsed_sentence.iter_tokens()):\n",
    "            # no markdown required for non-textual sentences\n",
    "            continue\n",
    "        for token in parsed_sentence.iter_tokens():\n",
    "            token_dict = token.to_dict()[0]\n",
    "            if token_dict[\"upos\"] == \"PUNCT\":\n",
    "                append_text(tokenized_sentence, token.text)\n",
    "            else:\n",
    "                tokenized_sentence.append(etree.Element(\"w\"))\n",
    "                word_token = last_child(tokenized_sentence)\n",
    "                word_token.append(etree.Element(\"ana\"))\n",
    "                ana = last_child(word_token)\n",
    "                if lang == LANG_SERBIAN:\n",
    "                    fill_text_and_lex(ana, token_dict, lang, latin_sentence == sentence.text)\n",
    "                else:\n",
    "                    fill_text_and_lex(ana, token_dict, lang, _)\n",
    "                annotate(lang, ana, token_dict)\n",
    "            if add_space_after(token_dict):\n",
    "                append_text(tokenized_sentence, \" \")\n",
    "\n",
    "        if \"variant_id\" in sentence.attrib.keys():\n",
    "            tokenized_sentence.attrib[\"variant_id\"] = sentence.attrib[\"variant_id\"]\n",
    "        sentence.getparent().replace(sentence, tokenized_sentence)\n",
    "        format_sentence(tokenized_sentence)\n",
    "\n",
    "def process_file(file_number, total_file_count, filename, pipelines, file_suff):\n",
    "    parsed_file = etree.parse(filename)\n",
    "    bar_format = f\"[{file_number + 1} / {total_file_count}] {str(filename)} \" + \"|{bar}| {percentage:3.0f}% {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]\"\n",
    "    for inner_tag in tqdm(parsed_file.find(\"body\"), bar_format=bar_format):\n",
    "        if inner_tag.tag == \"p\":    # some sentence pairs are enclosed in <p>...</p> tag, which should be kept\n",
    "            for child in inner_tag.getchildren():\n",
    "                process_para(child, pipelines)\n",
    "        elif inner_tag.tag == \"para\":\n",
    "            process_para(inner_tag, pipelines)\n",
    "        else:\n",
    "            print(f\"Incorrect sentence pair tag: {inner_tag.tag}\")\n",
    "\n",
    "    if not Path(STORE_RESULTS_AT).exists():\n",
    "        Path.mkdir(Path(STORE_RESULTS_AT))\n",
    "    recreate_original_filepath(filename)\n",
    "    with open(Path(STORE_RESULTS_AT).joinpath(add_suffix(filename, file_suff)), \"bw\") as annotated_file:\n",
    "        annotated_file.write(etree.tostring(parsed_file, encoding=\"utf-8\", xml_declaration=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 4.3: process files with \"standard\" pipelines"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for file_number, filename in enumerate(files_to_process):\n",
    "    total_file_count = len(files_to_process)\n",
    "    process_file(file_number, total_file_count, filename, pipelines, STD_SUFF)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 5: validate results and check the markdown"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [],
   "source": [
    "def check_para(para):\n",
    "    for sentence in para:\n",
    "        lang = sentence.get(\"lang\")\n",
    "        if lang not in LANG_CODE_NORMALIZED.keys() or sentence.text is None:\n",
    "            continue\n",
    "        lang = LANG_CODE_NORMALIZED[lang]\n",
    "        common_message_part = f\"[{file.name} ({lang}), para #{para.get('id')}]\"\n",
    "        if lang == LANG_SERBIAN:\n",
    "            for word_token in sentence:\n",
    "                ana = last_child(word_token)\n",
    "                if \"translit\" not in word_token.keys():\n",
    "                    print(f\"{common_message_part} missing transliterated version of the word \\\"{ana.tail}\\\"\")\n",
    "                    return False\n",
    "                if \"lex\" not in ana.keys():\n",
    "                    print(f\"{common_message_part} missing lexeme of the word \\\"{ana.tail}\\\"\")\n",
    "                    return False\n",
    "                if \"lex_translit\" not in ana.keys():\n",
    "                    print(f\"{common_message_part} missing transliterated version of lexeme of the word \\\"{ana.tail}\\\"\")\n",
    "                    return False\n",
    "                if \"gr\" not in ana.keys():\n",
    "                    print(f\"{common_message_part} missing grammatical features of the word \\\"{ana.tail}\\\"\")\n",
    "                    return False\n",
    "                else:\n",
    "                    for gr_prop in ana.get(\"gr\").split(\",\"):\n",
    "                        if not is_ncrl_feature(gr_prop):\n",
    "                            print(f\"{common_message_part} unknown grammatical feature in the word \\\"{ana.tail}\\\": {gr_prop}\")\n",
    "                            return False\n",
    "    return True\n",
    "\n",
    "def validate_files(file_list):\n",
    "    for source_file in file_list:\n",
    "        file = Path(STORE_RESULTS_AT).joinpath(source_file)\n",
    "        try:\n",
    "            parsed_file = etree.parse(file)\n",
    "            print(f\"✅ File {file} is well-formed.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ File {file} is NOT well-formed!\\n{e}\")\n",
    "            break\n",
    "\n",
    "        file_is_correctly_annotated = True\n",
    "        for inner_tag in parsed_file.find(\"body\"):\n",
    "            if inner_tag.tag == \"p\":\n",
    "                for child in inner_tag.getchildren():\n",
    "                    file_is_correctly_annotated = file_is_correctly_annotated and check_para(child)\n",
    "            elif inner_tag.tag == \"para\":\n",
    "                file_is_correctly_annotated = file_is_correctly_annotated and check_para(inner_tag)\n",
    "        if file_is_correctly_annotated:\n",
    "            print(f\"✅ File {file} is correctly annotated.\")\n",
    "        print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "validate_files(files_to_process)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 6.1: run \"non-standard\" pipelines"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for file_number, filename in enumerate(files_to_process[2:]):\n",
    "    total_file_count = len(files_to_process)\n",
    "    process_file(file_number, total_file_count, filename, pipelines_nonstd, NONSTD_SUFF)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 6.2: validate results and check the markdown"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "validate_files([add_suffix(filename, NONSTD_SUFF) for filename in files_to_process])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 6.3: merge \"standard\" and \"non-standard\" markdowns; list of alternative lexemes available [here](https://docs.google.com/spreadsheets/d/1Hl2ns1449xmOVByr5879jwjC3djMTiaZItPO5V6rtbg/edit), grammar attribute postprocessing sheet -- [here](https://docs.google.com/spreadsheets/d/1hAfJ3A4P7iaN24VE2AGhuNBB2NJ1Rv6PSf-wYucoeuw)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "merged_dir = Path(STORE_RESULTS_AT).joinpath(source_dir).joinpath(str(source_dir) + MERGED_SUFF)\n",
    "if not merged_dir.exists():\n",
    "    Path.mkdir(merged_dir)\n",
    "\n",
    "alt_lexemes_sheet_id = \"1Hl2ns1449xmOVByr5879jwjC3djMTiaZItPO5V6rtbg\"\n",
    "alt_lexemes_sheet_names = {LANG_SERBIAN: \"srp\"}\n",
    "\n",
    "def alt_lexemes_sheet_to_dict(lang):\n",
    "    alt_lexemes_sheet_dict = pd.read_csv(f\"https://docs.google.com/spreadsheets/d/{alt_lexemes_sheet_id}/gviz/tq?tqx=out:csv&sheet={alt_lexemes_sheet_names[lang]}\", na_filter= False).to_dict()\n",
    "    std = list(alt_lexemes_sheet_dict[\"std\"].values())\n",
    "    nonstd = list(alt_lexemes_sheet_dict[\"nonstd\"].values())\n",
    "    return {nonstd_mapping: std_mapping for std_mapping, nonstd_mapping in zip(std, nonstd)}\n",
    "\n",
    "alt_lexemes = {\n",
    "    LANG_SERBIAN: alt_lexemes_sheet_to_dict(LANG_SERBIAN),\n",
    "}\n",
    "\n",
    "alt_syls = {\n",
    "    \"je\": \"e\",\n",
    "    \"ije\": \"e\"\n",
    "}\n",
    "\n",
    "gr_postproc_sheet_id = \"1hAfJ3A4P7iaN24VE2AGhuNBB2NJ1Rv6PSf-wYucoeuw\"\n",
    "gr_postproc_sheet_names = {LANG_SERBIAN: \"srp\"}\n",
    "\n",
    "def gr_postproc_sheet_to_list(lang):\n",
    "    gr_postproc_sheet_dict = pd.read_csv(f\"https://docs.google.com/spreadsheets/d/{gr_postproc_sheet_id}/gviz/tq?tqx=out:csv&sheet={gr_postproc_sheet_names[lang]}\", na_filter= False).to_dict()\n",
    "    replace = list(gr_postproc_sheet_dict[\"replace\"].values())\n",
    "    _with = list(gr_postproc_sheet_dict[\"with\"].values())\n",
    "    if_lex_nonstd_matches_regex = list(gr_postproc_sheet_dict[\"if_lex_nonstd_matches_regex\"].values())\n",
    "    else_remove = list(gr_postproc_sheet_dict[\"else_remove\"].values())\n",
    "    return list(zip(replace, _with, if_lex_nonstd_matches_regex, else_remove))\n",
    "\n",
    "gr_postproc = {\n",
    "    LANG_SERBIAN: gr_postproc_sheet_to_list(LANG_SERBIAN),\n",
    "}\n",
    "\n",
    "def add_std_lexeme(std_latin, nonstd_latin):\n",
    "    return any(nonstd_latin.replace(key, value) == std_latin for key, value in alt_syls.items())\\\n",
    "           or ((nonstd_latin in alt_lexemes[LANG_SERBIAN].keys()) and (alt_lexemes[LANG_SERBIAN][nonstd_latin] == std_latin))\n",
    "\n",
    "def process_lex_diff(ana_std, ana_nonstd):\n",
    "    nonstd_latin = cyrtranslit.to_latin(ana_nonstd.get(\"lex\"), \"sr\")\n",
    "    std_latin = cyrtranslit.to_latin(ana_std.get(\"lex\"), \"sr\")\n",
    "    if add_std_lexeme(std_latin, nonstd_latin):\n",
    "        ana_std.addprevious(etree.Element(\"ana\", attrib={\"lex\": ana_nonstd.get('lex'), \"lex_translit\": ana_nonstd.get('lex_translit'), \"gr\": ana_std.get(\"gr\")}))\n",
    "    # else:\n",
    "    #     print(ana_std.get(\"lex\"), ana_nonstd.get(\"lex\"))\n",
    "\n",
    "def postprocess_gr(lang, nonstd_gr_split, ana_std):\n",
    "    for tok_to_replace, _with, cond_regex, tok_to_remove in gr_postproc[lang]:\n",
    "        if (tok_to_replace in nonstd_gr_split) and (tok_to_remove in nonstd_gr_split):\n",
    "            if re.match(cond_regex, cyrtranslit.to_latin(ana_std.get(\"lex\"), \"sr\")):\n",
    "                ana_std.attrib[\"gr\"] = \",\".join([prop if prop != tok_to_replace else _with for prop in nonstd_gr_split])\n",
    "            else:\n",
    "                ana_std.attrib[\"gr\"] = \",\".join([prop for prop in nonstd_gr_split if prop != tok_to_remove])\n",
    "\n",
    "def replace_path_segment(file, source, target):\n",
    "    return str(file).replace(source, target)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def merge_sentences(lang, se_std, se_nonstd):\n",
    "    for wt_std, wt_nonstd in zip(se_std.findall(\"w\"), se_nonstd.findall(\"w\")):\n",
    "        if lang == LANG_SERBIAN:\n",
    "            ana_std = last_child(wt_std)\n",
    "            ana_nonstd = last_child(wt_nonstd)\n",
    "            if ana_std.get(\"lex\").lower() != ana_nonstd.get(\"lex\").lower():\n",
    "                process_lex_diff(ana_std, ana_nonstd)\n",
    "\n",
    "            std_gr = ana_std.get(\"gr\").split(\",\")\n",
    "            nonstd_gr = ana_nonstd.get(\"gr\").split(\",\")\n",
    "            if (len(std_gr) != len(nonstd_gr)) or (len(set(std_gr) & set(nonstd_gr)) != len(std_gr)):\n",
    "                if set(nonstd_gr) != {\"INTJ\"}:\n",
    "                    ana_std.attrib[\"gr\"] = ana_nonstd.get(\"gr\")\n",
    "                continue # non-standard model sometimes incorrectly assigns \"INTJ\", then standard model's prediction should be prioritized\n",
    "            postprocess_gr(lang, nonstd_gr, ana_std)\n",
    "        else:\n",
    "            raise Exception(f\"actions for language={lang} are not defined!\")\n",
    "\n",
    "for file in files_to_process:\n",
    "    file_std = Path(STORE_RESULTS_AT).joinpath(add_suffix(file, STD_SUFF))\n",
    "    file_nonstd = Path(STORE_RESULTS_AT).joinpath(add_suffix(file, NONSTD_SUFF))\n",
    "\n",
    "    parsed_file_std = etree.parse(file_std)\n",
    "    parsed_file_nonstd = etree.parse(file_nonstd) # in-place merge will be performed, results will be saved to a new file\n",
    "    for lang in pipelines_nonstd.keys():\n",
    "        sentences_std = [se for se in parsed_file_std.findall(\".//se\") if se.get(\"lang\") == lang]\n",
    "        sentences_nonstd = [se for se in parsed_file_nonstd.findall(\".//se\") if se.get(\"lang\") == lang]\n",
    "        bar_format = \"Merging file: \" + str(file_nonstd.name) + \" |{bar}| {percentage:3.0f}% {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]\"\n",
    "        for se_std, se_nonstd in tqdm(list(zip(sentences_std, sentences_nonstd)), bar_format=bar_format):\n",
    "            merge_sentences(lang, se_std, se_nonstd)\n",
    "\n",
    "    merged_file_path = Path(Path(STORE_RESULTS_AT).joinpath(merged_dir))\n",
    "    recreate_original_filepath(merged_file_path)\n",
    "    with open(merged_dir.joinpath(add_suffix(file, MERGED_SUFF).name), \"bw\") as merged_file:\n",
    "        merged_file.write(etree.tostring(parsed_file_std, encoding=\"utf-8\", xml_declaration=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 7: check well-formedness of the results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "validate_files([replace_path_segment(merged_dir.joinpath(add_suffix(filename, MERGED_SUFF).name), STORE_RESULTS_AT + \"\\\\\", \"\") for filename in files_to_process])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
