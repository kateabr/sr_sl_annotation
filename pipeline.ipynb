{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Step 1: run `pip install classla` to install [classla](https://github.com/clarinsi/classla)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 2: download standard models and initialize pipelines"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import classla\n",
    "\n",
    "# slovenian\n",
    "classla.download('sl')\n",
    "sl_nlp = classla.Pipeline('sl')\n",
    "\n",
    "# serbian\n",
    "classla.download('sr')\n",
    "sr_nlp = classla.Pipeline('sr')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 3: specify input directories and match them with corresponding pipeline; check well-formedness of the files and correct if needed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from xml.sax.handler import ContentHandler\n",
    "from xml.sax import make_parser\n",
    "from lxml import etree\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_data = {\"source_sl\": sl_nlp,\n",
    "              \"source_sr\": sr_nlp}\n",
    "\n",
    "def parsefile(file):\n",
    "    parser = make_parser()\n",
    "    parser.setContentHandler(ContentHandler())\n",
    "    parser.parse(file)\n",
    "\n",
    "\n",
    "for directory in input_data.keys():\n",
    "    for filename in Path(directory).iterdir():\n",
    "        try:\n",
    "            etree.parse(filename)\n",
    "            print(f\"✅ File {filename} is well-formed.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ File {filename} is NOT well-formed!\\n{e}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 4: process files (no changes required)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import cyrtranslit\n",
    "\n",
    "known_keys = [\"feats\"]\n",
    "keys_to_ignore = [\"id\", \"text\", \"lemma\", \"upos\", \"xpos\", \"head\", \"ner\", \"deprel\"]\n",
    "\n",
    "def add_space_after(token_dict):\n",
    "    return not (\"misc\" in token_dict.keys() and \"SpaceAfter=No\" in token_dict[\"misc\"])\n",
    "\n",
    "def tidy(property_string):\n",
    "    lower = property_string.lower()\n",
    "    res = \"\"\n",
    "    props = {pair.split(\"=\")[0]: pair.split(\"=\")[-1] for pair in lower.split(\"|\")}\n",
    "    if \"gender\" in props.keys() and \"number\" in props.keys():\n",
    "        props[props[\"gender\"]] = props[\"number\"]\n",
    "        props.pop(\"gender\")\n",
    "        props.pop(\"number\")\n",
    "    for key, value in props.items():\n",
    "        if key != value:\n",
    "            res += f\",{key}={value}\"\n",
    "    return res\n",
    "\n",
    "def append_gr(ana_node, token_dict):\n",
    "    for key, value in token_dict.items():\n",
    "        if key not in keys_to_ignore:\n",
    "            if key not in known_keys:\n",
    "                if key != \"misc\" and value != \"SpaceAfter=No\":\n",
    "                    print(f\"{key} :: {value} in {token_dict}\")\n",
    "            else:\n",
    "                ana_node.attrib[\"gr\"] += tidy(value.lower())\n",
    "\n",
    "def append_text(root, text):\n",
    "    if len(root.getchildren()) > 0:\n",
    "        node = last_child(root)\n",
    "        if node.tail is None:\n",
    "            node.tail = text\n",
    "        else:\n",
    "            node.tail += text\n",
    "    else:\n",
    "        if root.text is None:\n",
    "            root.text = text\n",
    "        else:\n",
    "            root.text += text\n",
    "\n",
    "def last_child(node):\n",
    "    return node.getchildren()[-1]\n",
    "\n",
    "for directory, pipeline in input_data.items():\n",
    "    for filename in Path(directory).iterdir():\n",
    "        parsed_file = etree.parse(filename)\n",
    "        bar_format = \"File: \" + filename.name + \" |{bar}| {percentage:3.0f}% {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]\"\n",
    "        for para in tqdm(parsed_file.find(\"body\")[2:], bar_format=bar_format):\n",
    "            if para.tag != \"para\":\n",
    "                print(f\"Incorrect sentence pair tag: {para.tag}\")\n",
    "            for sentence in para:\n",
    "                if sentence.tag != \"se\":\n",
    "                    print(f\"Incorrect sentence tag: {sentence.tag}\")\n",
    "                if sentence.get(\"lang\") == \"rus\" or sentence.text is None:\n",
    "                    continue\n",
    "                tokenized_sentence = etree.Element(\"se\", attrib={\"lang\": sentence.get(\"lang\")})\n",
    "                if sentence.get(\"lang\") == \"srp\":\n",
    "                    parsed_sentence = pipeline(cyrtranslit.to_latin(sentence.text, \"sr\"))\n",
    "                else:\n",
    "                    parsed_sentence = pipeline(sentence.text)\n",
    "                for token in parsed_sentence.iter_tokens():\n",
    "                    token_dict = token.to_dict()[0]\n",
    "                    if token_dict[\"upos\"] == \"PUNCT\":\n",
    "                        append_text(tokenized_sentence, token.text)\n",
    "                    else:\n",
    "                        tokenized_sentence.append(etree.Element(\"w\"))\n",
    "                        word_token = last_child(tokenized_sentence)\n",
    "                        word_token.append(etree.Element(\"ana\", attrib={\"lex\": token_dict[\"lemma\"], \"gr\": token_dict[\"upos\"]}))\n",
    "                        ana = last_child(word_token)\n",
    "                        ana.tail = token_dict[\"text\"]\n",
    "                        append_gr(ana, token_dict)\n",
    "                    if add_space_after(token_dict):\n",
    "                        append_text(tokenized_sentence, \" \")\n",
    "\n",
    "                sentence.getparent().replace(sentence, tokenized_sentence)\n",
    "\n",
    "        if not Path(\"annotated\").exists():\n",
    "            Path.mkdir(Path(\"annotated\"))\n",
    "        if not Path(\"annotated\").joinpath(directory).exists():\n",
    "            Path.mkdir(Path(\"annotated\").joinpath(directory))\n",
    "        with open(Path(\"annotated\").joinpath(filename), \"bw\") as annotated_file:\n",
    "            annotated_file.write(etree.tostring(parsed_file, encoding=\"utf-8\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 5: check well-formedness of the results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for directory in input_data.keys():\n",
    "    for filename in Path(\"annotated\").joinpath(directory).iterdir():\n",
    "        if not filename.is_file():\n",
    "            continue\n",
    "        try:\n",
    "            etree.parse(filename)\n",
    "            print(f\"✅ File {filename} is well-formed.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ File {filename} is NOT well-formed!\\n{e}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}