{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Step 1: run `pip install classla` to install [classla](https://github.com/clarinsi/classla)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 2: download standard models and initialize pipelines"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import classla\n",
    "\n",
    "# slovenian\n",
    "classla.download('sl')\n",
    "sl_nlp = classla.Pipeline('sl')\n",
    "\n",
    "# serbian\n",
    "classla.download('sr')\n",
    "sr_nlp = classla.Pipeline('sr')\n",
    "\n",
    "# serbian non-standard\n",
    "classla.download('sr', type='nonstandard')\n",
    "sr_nlp_nonstandard = classla.Pipeline('sr', type='nonstandard')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 3: specify input directories and match them with corresponding pipeline; check well-formedness of the files and correct if needed (only well-formed xmls will be processed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "sl_base = \"source_sl\"\n",
    "sr_base = \"source_sr\"\n",
    "sr_directory_std = sr_base\n",
    "sr_directory_nonstd = sr_base + \"_nonstandard\"\n",
    "\n",
    "MERGED_SUFF = \"_merged\"\n",
    "targ_directory_merged = sr_base + MERGED_SUFF\n",
    "\n",
    "input_data = {\n",
    "    sl_base: sl_nlp,\n",
    "    sr_directory_std: sr_nlp,\n",
    "    sr_directory_nonstd: sr_nlp_nonstandard\n",
    "}\n",
    "\n",
    "files_to_process = defaultdict(list)\n",
    "\n",
    "def check_directory(directory_nested):\n",
    "    for file in Path(directory_nested).iterdir():\n",
    "        if not file.is_file():\n",
    "            check_directory(file)\n",
    "            continue\n",
    "        try:\n",
    "            etree.parse(file)\n",
    "            print(f\"✅ File {file} is well-formed.\")\n",
    "            files_to_process[directory].append(file)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ File {file} is NOT well-formed!\\n{e}\")\n",
    "\n",
    "for directory in input_data.keys():\n",
    "    check_directory(directory)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 4.1: load [markdown mappings](https://docs.google.com/spreadsheets/d/1PiNjHQ7NoJyYLiTin4VehWd21Tc7LLdoCr52-t9_EjY) and initialize mapping function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "LANG_SERBIAN = \"srp\"\n",
    "LANG_SLOVENIAN = \"slv\"\n",
    "LANG_RUSSIAN = \"rus\"\n",
    "\n",
    "LANG_CODE_NORMALIZED = {\n",
    "    LANG_SLOVENIAN: LANG_SLOVENIAN,\n",
    "    \"sln\": LANG_SLOVENIAN,\n",
    "    LANG_SERBIAN: LANG_SERBIAN,\n",
    "    LANG_RUSSIAN: LANG_RUSSIAN\n",
    "}\n",
    "\n",
    "mapping_sheet_id = \"1PiNjHQ7NoJyYLiTin4VehWd21Tc7LLdoCr52-t9_EjY\"\n",
    "mapping_sheet_names = {LANG_SLOVENIAN: \"sl_ru\", LANG_SERBIAN: \"srp_ru\"}\n",
    "\n",
    "def mapping_sheet_to_dict(lang):\n",
    "    mapping_sheet_dict = pd.read_csv(f\"https://docs.google.com/spreadsheets/d/{mapping_sheet_id}/gviz/tq?tqx=out:csv&sheet={mapping_sheet_names[lang]}\", na_filter= False).to_dict()\n",
    "    src = list(mapping_sheet_dict[\"classla\"].values())\n",
    "    trg = list(map(lambda x: x if len(x) > 0 else None, list(mapping_sheet_dict[\"ncrl\"].values())))\n",
    "    return {src_mapping: trg_mapping for src_mapping, trg_mapping in zip(src, trg)}\n",
    "\n",
    "gr_map = {\n",
    "    LANG_SLOVENIAN: mapping_sheet_to_dict(LANG_SLOVENIAN),\n",
    "    LANG_SERBIAN: mapping_sheet_to_dict(LANG_SERBIAN),\n",
    "}\n",
    "\n",
    "additional_ncrl_features = {\"ANUM\", \"NONLEX\", \"gvrn:acc\", \"gvrn:dat\", \"gvrn:gen\", \"gvrn:ins\", \"gvrn:loc\", \"gvrn:nom\", \"gvrn:voc\"}\n",
    "ncrl_features = set([])\n",
    "for dct in gr_map.values():\n",
    "    ncrl_features = ncrl_features.union(dct.values())\n",
    "ncrl_features = additional_ncrl_features.union(sum([val.split(\",\") for val in ncrl_features if val is not None], []))\n",
    "\n",
    "def get_mapping(lang, prop):\n",
    "    return gr_map[lang][prop]\n",
    "\n",
    "def is_mapped(lang, prop):\n",
    "    return prop in gr_map[lang].keys()\n",
    "\n",
    "def is_ncrl_feature(prop):\n",
    "    return prop in ncrl_features\n",
    "\n",
    "\n",
    "def map_to_ncrl(lang, pos, attrs):\n",
    "    if not is_mapped(lang, pos):\n",
    "        print(f\"[{lang}] unknown POS: {pos}\")\n",
    "        res_pos = pos\n",
    "    else:\n",
    "        res_pos = get_mapping(lang, pos)\n",
    "    if attrs is None:\n",
    "        return res_pos, None\n",
    "    res_attrs = set([])\n",
    "    for attr_pair_str in attrs.lower().split(\"|\"):\n",
    "        if attr_pair_str == \"numtype=ord\" and res_pos == \"A\":\n",
    "            res_pos = \"ANUM\"\n",
    "        elif attr_pair_str == \"foreign=yes\":\n",
    "            res_pos = \"NONLEX\"\n",
    "        elif attr_pair_str.startswith(\"case=\") and res_pos == \"PR\":\n",
    "            res_attrs.add(f\"gvrn:{get_mapping(lang, attr_pair_str)}\")\n",
    "        elif not is_mapped(lang, attr_pair_str):\n",
    "            print(f\"[{lang}] unmapped attribute pair: {attr_pair_str}\")\n",
    "            res_attrs.add(attr_pair_str)\n",
    "        elif (mapping := get_mapping(lang, attr_pair_str)) is not None:\n",
    "            res_attrs.add(mapping)\n",
    "    if len(res_attrs) > 0:\n",
    "        return res_pos, \",\".join(res_attrs)\n",
    "    else:\n",
    "        return res_pos, None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 4.2: initialize file processing functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "import cyrtranslit\n",
    "from tqdm import tqdm\n",
    "\n",
    "# processed files will be exported to this folder\n",
    "STORE_RESULTS_AT = \"annotated\"\n",
    "\n",
    "known_keys = {\"feats\"}\n",
    "keys_to_ignore = {\"id\", \"text\", \"lemma\", \"upos\", \"xpos\", \"head\", \"ner\", \"deprel\"}\n",
    "non_textual_pos = {\"PUNCT\", \"X\", \"SYM\"}\n",
    "\n",
    "def add_space_after(token_dict):\n",
    "    return not (\"misc\" in token_dict.keys() and \"SpaceAfter=No\" in token_dict[\"misc\"])\n",
    "\n",
    "def annotate(lang, ana, token_dict):\n",
    "    for key, value in token_dict.items():\n",
    "        if key not in keys_to_ignore and key not in known_keys and (key != \"misc\" and value != \"SpaceAfter=No\"):\n",
    "            print(f\"{key} :: {value} in {token_dict}\")\n",
    "    if \"feats\" in token_dict.keys():\n",
    "        pos, attrs = map_to_ncrl(lang, token_dict[\"upos\"], token_dict[\"feats\"])\n",
    "        if attrs is not None:\n",
    "            ana.attrib[\"gr\"] = f\"{pos},{attrs}\"\n",
    "        else:\n",
    "            ana.attrib[\"gr\"] = pos\n",
    "    else:\n",
    "        pos, _ = map_to_ncrl(lang, token_dict[\"upos\"], None)\n",
    "        ana.attrib[\"gr\"] = pos\n",
    "\n",
    "def append_text(root, text):\n",
    "    if len(root.getchildren()) > 0:\n",
    "        node = last_child(root)\n",
    "        if node.tail is None:\n",
    "            node.tail = text\n",
    "        else:\n",
    "            node.tail += text\n",
    "    else:\n",
    "        if root.text is None:\n",
    "            root.text = text\n",
    "        else:\n",
    "            root.text += text\n",
    "\n",
    "def last_child(node):\n",
    "    return node.getchildren()[-1]\n",
    "\n",
    "def get_word(token):\n",
    "    return last_child(token).tail.strip()\n",
    "\n",
    "def fill_text_and_lex(ana, token_dict, lang, is_latin):\n",
    "    word_token = ana.getparent()\n",
    "    if lang == LANG_SERBIAN:\n",
    "        if is_latin:\n",
    "            ana.tail = token_dict[\"text\"]\n",
    "            word_token.attrib[\"translit\"] = cyrtranslit.to_cyrillic(token_dict[\"text\"], \"sr\")\n",
    "            ana.attrib[\"lex\"] = token_dict[\"lemma\"]\n",
    "            ana.attrib[\"lex_translit\"] = cyrtranslit.to_cyrillic(token_dict[\"lemma\"], \"sr\")\n",
    "        else:\n",
    "            ana.tail = cyrtranslit.to_cyrillic(token_dict[\"text\"], \"sr\")\n",
    "            word_token.attrib[\"translit\"] = token_dict[\"text\"]\n",
    "            ana.attrib[\"lex\"] = cyrtranslit.to_cyrillic(token_dict[\"lemma\"], \"sr\")\n",
    "            ana.attrib[\"lex_translit\"] = token_dict[\"lemma\"]\n",
    "    else:\n",
    "        ana.tail = token_dict[\"text\"]\n",
    "        ana.attrib[\"lex\"] = token_dict[\"lemma\"]\n",
    "\n",
    "def recreate_original_filepath(filename):\n",
    "    try:\n",
    "        if not Path(STORE_RESULTS_AT).joinpath(filename.parent).exists():\n",
    "            new_filepath = Path(STORE_RESULTS_AT)\n",
    "            for part in filename.parts:\n",
    "                if part == filename.name:\n",
    "                    break\n",
    "                new_filepath = new_filepath.joinpath(part)\n",
    "                if not Path(new_filepath).exists():\n",
    "                    Path.mkdir(new_filepath)\n",
    "    except Exception as _:\n",
    "        Path.mkdir(Path(STORE_RESULTS_AT).joinpath(filename.parent))\n",
    "\n",
    "def format_sentence(sentence):\n",
    "    previous_variant = sentence.getprevious()\n",
    "    if previous_variant is not None:\n",
    "        if previous_variant.tail is None:\n",
    "            previous_variant.tail = \"\\n    \"\n",
    "        else:\n",
    "            previous_variant.tail = previous_variant.tail.strip() + \"\\n    \"\n",
    "\n",
    "def process_file(file_number, total_file_count, filename, pipeline):\n",
    "    parsed_file = etree.parse(filename)\n",
    "    bar_format = f\"[{file_number + 1} / {total_file_count}] {str(filename)} \" + \"|{bar}| {percentage:3.0f}% {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]\"\n",
    "    for para in tqdm(parsed_file.find(\"body\"), bar_format=bar_format):\n",
    "        if para.tag != \"para\":\n",
    "            print(f\"Incorrect sentence pair tag: {para.tag}\")\n",
    "        for sentence in para:\n",
    "            if sentence.tag != \"se\":\n",
    "                print(f\"Incorrect sentence tag: {sentence.tag}\")\n",
    "            lang = LANG_CODE_NORMALIZED[sentence.get(\"lang\")]\n",
    "            if lang == LANG_RUSSIAN or sentence.text is None:\n",
    "                format_sentence(sentence)\n",
    "                continue\n",
    "            tokenized_sentence = etree.Element(\"se\", attrib={\"lang\": lang})\n",
    "            if lang == LANG_SERBIAN:\n",
    "                latin_sentence = cyrtranslit.to_latin(sentence.text, \"sr\")\n",
    "                parsed_sentence = pipeline(latin_sentence)\n",
    "                tokenized_sentence.attrib[\"has_translit\"] = \"true\"\n",
    "            else:\n",
    "                parsed_sentence = pipeline(sentence.text)\n",
    "            if all(token.to_dict()[0][\"upos\"] in non_textual_pos for token in parsed_sentence.iter_tokens()):\n",
    "                # no markdown required for non-textual sentences\n",
    "                continue\n",
    "            for token in parsed_sentence.iter_tokens():\n",
    "                token_dict = token.to_dict()[0]\n",
    "                if token_dict[\"upos\"] == \"PUNCT\":\n",
    "                    append_text(tokenized_sentence, token.text)\n",
    "                else:\n",
    "                    tokenized_sentence.append(etree.Element(\"w\"))\n",
    "                    word_token = last_child(tokenized_sentence)\n",
    "                    word_token.append(etree.Element(\"ana\"))\n",
    "                    ana = last_child(word_token)\n",
    "                    if lang == LANG_SERBIAN:\n",
    "                        fill_text_and_lex(ana, token_dict, lang, latin_sentence == sentence.text)\n",
    "                    else:\n",
    "                        fill_text_and_lex(ana, token_dict, lang, _)\n",
    "                    annotate(lang, ana, token_dict)\n",
    "                if add_space_after(token_dict):\n",
    "                    append_text(tokenized_sentence, \" \")\n",
    "\n",
    "            if \"variant_id\" in sentence.attrib.keys():\n",
    "                tokenized_sentence.attrib[\"variant_id\"] = sentence.attrib[\"variant_id\"]\n",
    "            sentence.getparent().replace(sentence, tokenized_sentence)\n",
    "            format_sentence(tokenized_sentence)\n",
    "\n",
    "    if not Path(STORE_RESULTS_AT).exists():\n",
    "        Path.mkdir(Path(STORE_RESULTS_AT))\n",
    "    recreate_original_filepath(filename)\n",
    "    with open(Path(STORE_RESULTS_AT).joinpath(filename), \"bw\") as annotated_file:\n",
    "        annotated_file.write(etree.tostring(parsed_file, encoding=\"utf-8\", xml_declaration=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 4.3: process files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for directory, pipeline in input_data.items():\n",
    "    total_file_count = len(files_to_process[directory])\n",
    "    for file_number, filename in enumerate(files_to_process[directory]):\n",
    "        process_file(file_number, total_file_count, filename, pipeline)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 5: check well-formedness of the results and validity of the markdown"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for directory in input_data.keys():\n",
    "    for source_file in files_to_process[directory]:\n",
    "        file = Path(STORE_RESULTS_AT).joinpath(source_file)\n",
    "        try:\n",
    "            parsed_file = etree.parse(file)\n",
    "            print(f\"✅ File {file} is well-formed.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ File {file} is NOT well-formed!\\n{e}\")\n",
    "            break\n",
    "\n",
    "        file_is_correctly_annotated = True\n",
    "        for para in parsed_file.find(\"body\"):\n",
    "            for sentence in para:\n",
    "                lang = sentence.get(\"lang\")\n",
    "                common_message_part = f\"[{file.name} ({lang}), para #{para.get('id')}]\"\n",
    "                if lang == LANG_RUSSIAN or sentence.text is None:\n",
    "                    continue\n",
    "                if lang == LANG_SERBIAN:\n",
    "                    for word_token in sentence:\n",
    "                        ana = last_child(word_token)\n",
    "                        if \"translit\" not in word_token.keys():\n",
    "                            print(f\"{common_message_part} missing transliterated version of the word \\\"{ana.tail}\\\"\")\n",
    "                            file_is_correctly_annotated = False\n",
    "                        if \"lex\" not in ana.keys():\n",
    "                            print(f\"{common_message_part} missing lexeme of the word \\\"{ana.tail}\\\"\")\n",
    "                            file_is_correctly_annotated = False\n",
    "                        if \"lex_translit\" not in ana.keys():\n",
    "                            print(f\"{common_message_part} missing transliterated version of lexeme of the word \\\"{ana.tail}\\\"\")\n",
    "                            file_is_correctly_annotated = False\n",
    "                        if \"gr\" not in ana.keys():\n",
    "                            print(f\"{common_message_part} missing grammatical features of the word \\\"{ana.tail}\\\"\")\n",
    "                            file_is_correctly_annotated = False\n",
    "                        else:\n",
    "                            for gr_prop in ana.get(\"gr\").split(\",\"):\n",
    "                                if not is_ncrl_feature(gr_prop):\n",
    "                                    print(f\"{common_message_part} unknown grammatical feature in the word \\\"{ana.tail}\\\": {gr_prop}\")\n",
    "                                    file_is_correctly_annotated = False\n",
    "        if file_is_correctly_annotated:\n",
    "            print(f\"✅ File {file} is correctly annotated.\")\n",
    "        print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 6: merge \"standard\" and \"non-standard\" files (serbian); list of alternative lexemes available [here](https://docs.google.com/spreadsheets/d/1Hl2ns1449xmOVByr5879jwjC3djMTiaZItPO5V6rtbg/edit), grammar attribute postprocessing sheet -- [here](https://docs.google.com/spreadsheets/d/1hAfJ3A4P7iaN24VE2AGhuNBB2NJ1Rv6PSf-wYucoeuw)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "if not Path(STORE_RESULTS_AT).absolute().joinpath(targ_directory_merged).exists():\n",
    "    Path.mkdir(Path(STORE_RESULTS_AT).joinpath(targ_directory_merged))\n",
    "\n",
    "alt_lexemes_sheet_id = \"1Hl2ns1449xmOVByr5879jwjC3djMTiaZItPO5V6rtbg\"\n",
    "alt_lexemes_sheet_names = {LANG_SERBIAN: \"srp\"}\n",
    "\n",
    "def alt_lexemes_sheet_to_dict(lang):\n",
    "    alt_lexemes_sheet_dict = pd.read_csv(f\"https://docs.google.com/spreadsheets/d/{alt_lexemes_sheet_id}/gviz/tq?tqx=out:csv&sheet={alt_lexemes_sheet_names[lang]}\", na_filter= False).to_dict()\n",
    "    std = list(alt_lexemes_sheet_dict[\"std\"].values())\n",
    "    nonstd = list(alt_lexemes_sheet_dict[\"nonstd\"].values())\n",
    "    return {nonstd_mapping: std_mapping for std_mapping, nonstd_mapping in zip(std, nonstd)}\n",
    "\n",
    "alt_lexemes = {\n",
    "    LANG_SERBIAN: alt_lexemes_sheet_to_dict(LANG_SERBIAN),\n",
    "}\n",
    "\n",
    "alt_syls = {\n",
    "    \"je\": \"e\",\n",
    "    \"ije\": \"e\"\n",
    "}\n",
    "\n",
    "gr_postproc_sheet_id = \"1hAfJ3A4P7iaN24VE2AGhuNBB2NJ1Rv6PSf-wYucoeuw\"\n",
    "gr_postproc_sheet_names = {LANG_SERBIAN: \"srp\"}\n",
    "\n",
    "def gr_postproc_sheet_to_list(lang):\n",
    "    gr_postproc_sheet_dict = pd.read_csv(f\"https://docs.google.com/spreadsheets/d/{gr_postproc_sheet_id}/gviz/tq?tqx=out:csv&sheet={gr_postproc_sheet_names[lang]}\", na_filter= False).to_dict()\n",
    "    replace = list(gr_postproc_sheet_dict[\"replace\"].values())\n",
    "    _with = list(gr_postproc_sheet_dict[\"with\"].values())\n",
    "    if_lex_nonstd_matches_regex = list(gr_postproc_sheet_dict[\"if_lex_nonstd_matches_regex\"].values())\n",
    "    else_remove = list(gr_postproc_sheet_dict[\"else_remove\"].values())\n",
    "    return list(zip(replace, _with, if_lex_nonstd_matches_regex, else_remove))\n",
    "\n",
    "gr_postproc = {\n",
    "    LANG_SERBIAN: gr_postproc_sheet_to_list(LANG_SERBIAN),\n",
    "}\n",
    "\n",
    "def add_std_lexeme(std_latin, nonstd_latin):\n",
    "    return any(nonstd_latin.replace(key, value) == std_latin for key, value in alt_syls.items())\\\n",
    "           or ((nonstd_latin in alt_lexemes[LANG_SERBIAN].keys()) and (alt_lexemes[LANG_SERBIAN][nonstd_latin] == std_latin))\n",
    "\n",
    "def process_lex_diff(ana_std, ana_nonstd):\n",
    "    nonstd_latin = cyrtranslit.to_latin(ana_nonstd.get(\"lex\"), \"sr\")\n",
    "    std_latin = cyrtranslit.to_latin(ana_std.get(\"lex\"), \"sr\")\n",
    "    if add_std_lexeme(std_latin, nonstd_latin):\n",
    "        ana_nonstd.addprevious(etree.Element(\"ana\", attrib={\"lex\": ana_std.get('lex'), \"lex_translit\": ana_std.get('lex_translit'), \"gr\": ana_nonstd.get(\"gr\")}))\n",
    "    # else:\n",
    "    #     print(ana_std.get(\"lex\"), ana_nonstd.get(\"lex\"))\n",
    "\n",
    "def postprocess_gr(lang, nonstd_gr_split, ana_nonstd):\n",
    "    for tok_to_replace, _with, cond_regex, tok_to_remove in gr_postproc[lang]:\n",
    "        if (tok_to_replace in nonstd_gr_split) and (tok_to_remove in nonstd_gr_split):\n",
    "            if re.match(cond_regex, cyrtranslit.to_latin(ana_nonstd.get(\"lex\"), \"sr\")):\n",
    "                ana_nonstd.attrib[\"gr\"] = \",\".join([prop if prop != tok_to_replace else _with for prop in nonstd_gr_split])\n",
    "            else:\n",
    "                ana_nonstd.attrib[\"gr\"] = \",\".join([prop for prop in nonstd_gr_split if prop != tok_to_remove])\n",
    "\n",
    "def replace_path_segment(file, source, target):\n",
    "    return str(file).replace(source, target)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for file in files_to_process[sr_directory_std]:\n",
    "    file_std = Path(STORE_RESULTS_AT).joinpath(file)\n",
    "    file_nonstd = Path(STORE_RESULTS_AT).joinpath(replace_path_segment(file, sr_directory_std, sr_directory_nonstd))\n",
    "\n",
    "    parsed_file_std = etree.parse(file_std)\n",
    "    parsed_file_nonstd = etree.parse(file_nonstd) # in-place merge will be performed, results will be saved to a new file\n",
    "    sentences_std = [se for se in parsed_file_std.findall(\".//se\") if se.get(\"lang\") == LANG_SERBIAN]\n",
    "    sentences_nonstd = [se for se in parsed_file_nonstd.findall(\".//se\") if se.get(\"lang\") == LANG_SERBIAN]\n",
    "    bar_format = \"Merging file: \" + str(file_nonstd.name) + \" |{bar}| {percentage:3.0f}% {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]\"\n",
    "    for para_id, (se_std, se_nonstd) in tqdm(list(enumerate(zip(sentences_std, sentences_nonstd))), bar_format=bar_format):\n",
    "        for wt_std, wt_nonstd in zip(se_std.findall(\"w\"), se_nonstd.findall(\"w\")):\n",
    "            ana_std = last_child(wt_std)\n",
    "            ana_nonstd = last_child(wt_nonstd)\n",
    "            if ana_std.get(\"lex\").lower() != ana_nonstd.get(\"lex\").lower():\n",
    "                process_lex_diff(ana_std, ana_nonstd)\n",
    "\n",
    "            std_gr = ana_std.get(\"gr\").split(\",\")\n",
    "            nonstd_gr = ana_nonstd.get(\"gr\").split(\",\")\n",
    "            if (len(std_gr) != len(nonstd_gr)) or (len(set(std_gr) & set(nonstd_gr)) != len(std_gr)):\n",
    "                if set(nonstd_gr) == {\"INTJ\"}:\n",
    "                    ana_nonstd.attrib[\"gr\"] = ana_std.get(\"gr\") # non-standard model sometimes incorrectly assigns \"INTJ\", then standard model's prediction should be prioritized\n",
    "                continue # keeping the non-standard model's prediction otherwise\n",
    "\n",
    "            postprocess_gr(LANG_SERBIAN, nonstd_gr, ana_nonstd)\n",
    "\n",
    "    merged_file_path = Path(replace_path_segment(file, sr_directory_std, targ_directory_merged))\n",
    "    recreate_original_filepath(merged_file_path)\n",
    "    with open(Path(STORE_RESULTS_AT).joinpath(merged_file_path), \"bw\") as merged_file:\n",
    "        merged_file.write(etree.tostring(parsed_file_nonstd, encoding=\"utf-8\", xml_declaration=True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 7: check well-formedness of the results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for source_file in files_to_process[sr_directory_std]:\n",
    "    file = Path(STORE_RESULTS_AT).joinpath(replace_path_segment(source_file, sr_directory_std, targ_directory_merged))\n",
    "    try:\n",
    "        parsed_file = etree.parse(file)\n",
    "        print(f\"✅ File {file} is well-formed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ File {file} is NOT well-formed!\\n{e}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
